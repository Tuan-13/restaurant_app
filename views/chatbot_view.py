# views/chatbot_view.py
import streamlit as st
from google import genai
from google.genai import types
from utils import get_text

def render_chatbot_tab(lang):
    c1, c2 = st.columns([3, 1])
    with c1:
        st.title(get_text("chatbot_title", lang))
        st.caption(get_text("chatbot_caption", lang))
    with c2:
        if st.button(get_text("clear_chat", lang), use_container_width=True):
            st.session_state.gemini_messages = []
            st.rerun()

    api_key = st.secrets["GEMINI_API_KEY"] if "GEMINI_API_KEY" in st.secrets else None
        
    if api_key:
        client = genai.Client(api_key=api_key) 
    else:
        st.warning(get_text("no_api_key", lang))
        client = None

    # RAG Context
    search_context = ""
    if "search_results" in st.session_state and st.session_state.search_results:
        results = st.session_state.search_results
        search_context = "\n\nDÆ¯á»šI ÄÃ‚Y LÃ€ DANH SÃCH CÃC QUÃN Ä‚N NGÆ¯á»œI DÃ™NG ÄANG TÃŒM THáº¤Y TRÃŠN Báº¢N Äá»’:\n"
        for i, r in enumerate(results):
            search_context += f"{i+1}. TÃªn: {r['name']} | GiÃ¡: {r['price']} | Loáº¡i: {r['cuisine']} | Khoáº£ng cÃ¡ch: {int(r['distance_sort'])}m\n"
        search_context += "\n(HÃ£y sá»­ dá»¥ng thÃ´ng tin nÃ y Ä‘á»ƒ tráº£ lá»i náº¿u ngÆ°á»i dÃ¹ng há»i vá» cÃ¡c quÃ¡n Ä‘Ã£ tÃ¬m tháº¥y. Náº¿u khÃ´ng, hÃ£y tÆ° váº¥n chung)."
    else:
        search_context = "\n(NgÆ°á»i dÃ¹ng chÆ°a thá»±c hiá»‡n tÃ¬m kiáº¿m nÃ o trÃªn báº£n Ä‘á»“)."

    system_instruction_text = f"""
    Báº¡n lÃ  má»™t chuyÃªn gia áº©m thá»±c Ä‘á»‹a phÆ°Æ¡ng am hiá»ƒu vÃ  thÃ¢n thiá»‡n (Foodie Guide).
    Nhiá»‡m vá»¥ cá»§a báº¡n:
    1. TÆ° váº¥n mÃ³n Äƒn, giáº£i thÃ­ch vÄƒn hÃ³a áº©m thá»±c Viá»‡t Nam.
    2. PhÃ¢n tÃ­ch danh sÃ¡ch quÃ¡n Äƒn mÃ  ngÆ°á»i dÃ¹ng tÃ¬m Ä‘Æ°á»£c (náº¿u cÃ³).
    3. ÄÆ°a ra gá»£i Ã½ dá»±a trÃªn sá»Ÿ thÃ­ch (cay, ráº», view Ä‘áº¹p, v.v.).
    
    Phong cÃ¡ch tráº£ lá»i: Ngáº¯n gá»n, dÃ¹ng emoji ğŸœ, thÃ¢n thiá»‡n, Ä‘á»‹nh dáº¡ng Markdown Ä‘áº¹p máº¯t.
    {search_context}
    """

    # Suggestion Chips
    if not st.session_state.get("gemini_messages"):
        st.info(get_text("suggestion_header", lang))
        cols = st.columns(3)
        if cols[0].button(get_text("chip_analyze", lang)):
            prompt = "Dá»±a trÃªn danh sÃ¡ch cÃ¡c quÃ¡n vá»«a tÃ¬m tháº¥y, hÃ£y phÃ¢n tÃ­ch Æ°u nhÆ°á»£c Ä‘iá»ƒm cá»§a chÃºng giÃºp tÃ´i."
        elif cols[1].button(get_text("chip_side_dish", lang)):
            dish = st.session_state.get('dish_input', 'nÃ y')
            prompt = f"MÃ³n {dish} thÆ°á»ng Äƒn kÃ¨m vá»›i gÃ¬ vÃ  Äƒn nhÆ° tháº¿ nÃ o cho Ä‘Ãºng Ä‘iá»‡u?"
        elif cols[2].button(get_text("chip_cheapest", lang)):
            prompt = "Trong danh sÃ¡ch trÃªn, quÃ¡n nÃ o cÃ³ giÃ¡ ráº» nháº¥t vÃ  gáº§n tÃ´i nháº¥t?"
        else:
            prompt = None
    else:
        prompt = None

    # Chat History
    if "gemini_messages" not in st.session_state:
        st.session_state["gemini_messages"] = [] 

    for message in st.session_state.gemini_messages:
        role = "user" if message.role == "user" else "assistant"
        content = message.parts[0].text
        with st.chat_message(role):
            st.markdown(content)

    # Input Handling
    user_input = st.chat_input(get_text("chat_placeholder", lang))
    final_prompt = prompt if prompt else user_input

    if final_prompt:
        if not client:
            st.error(get_text("no_api_key", lang))
        else:
            user_message = types.Content(role="user", parts=[types.Part(text=final_prompt)])
            st.session_state.gemini_messages.append(user_message)

            with st.chat_message("user"):
                st.markdown(final_prompt)

            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                full_response = ""
                    
                try:
                    response = client.models.generate_content_stream(
                        model="gemini-2.5-flash", 
                        contents=st.session_state.gemini_messages,
                        config=types.GenerateContentConfig(
                            system_instruction=system_instruction_text,
                            temperature=0.7
                        )
                    )
                        
                    for chunk in response:
                        if chunk.text:
                            full_response += chunk.text
                            message_placeholder.markdown(full_response + "â–Œ")
                        
                    message_placeholder.markdown(full_response)
                    
                    assistant_message = types.Content(role="model", parts=[types.Part(text=full_response)])
                    st.session_state.gemini_messages.append(assistant_message)

                except Exception as e:
                    st.error(f"ÄÃ£ xáº£y ra lá»—i: {e}")