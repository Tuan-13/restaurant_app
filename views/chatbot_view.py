# views/chatbot_view.py
import streamlit as st
from google import genai
from google.genai import types
from translate import get_text

# --- T·ªêI ∆ØU H√ìA: Cache Client ---
# Gi√∫p kh√¥ng ph·∫£i kh·ªüi t·∫°o l·∫°i k·∫øt n·ªëi m·ªói khi ng∆∞·ªùi d√πng t∆∞∆°ng t√°c, l√†m app m∆∞·ª£t h∆°n.
@st.cache_resource
def get_genai_client(api_key):
    return genai.Client(api_key=api_key)

def render_chatbot_tab(lang):
    # 1. Giao di·ªán Header hi·ªán ƒë·∫°i
    st.markdown(f"""
    <div style="
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1.5rem 2rem;
        border-radius: 16px;
        margin-bottom: 1.5rem;
        box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);
    ">
        <div style="display: flex; align-items: center; justify-content: space-between;">
            <div>
                <div style="
                    font-family: 'Poppins', sans-serif;
                    font-size: 1.5rem;
                    font-weight: 600;
                    color: white;
                    display: flex;
                    align-items: center;
                    gap: 0.75rem;
                ">
                    <span style="font-size: 1.75rem;">ü§ñ</span>
                    {get_text('chatbot_title', lang)}
                </div>
                <div style="
                    color: rgba(255,255,255,0.8);
                    font-size: 0.9rem;
                    margin-top: 0.25rem;
                ">{get_text('chatbot_caption', lang)}</div>
            </div>
        </div>
    </div>
    """, unsafe_allow_html=True)

    # N√∫t x√≥a chat
    col_spacer, col_btn = st.columns([4, 1])
    with col_btn:
        if st.button(get_text("clear_chat", lang), use_container_width=True):
            st.session_state.gemini_messages = []
            st.rerun()

    # 2. Kh·ªüi t·∫°o Client
    # L·∫•y API Key t·ª´ secrets (b·∫£o m·∫≠t h∆°n hardcode)
    api_key = st.secrets.get("GOOGLE_AI_API_KEY")
    
    if api_key:
        client = get_genai_client(api_key)
    else:
        st.warning(get_text("no_api_key", lang))
        client = None

    # 3. Chu·∫©n b·ªã Context (D·ªØ li·ªáu qu√°n ƒÉn)
    # Logic: Ch·ªâ l·∫•y Top 5 qu√°n ƒë·ªÉ ƒë∆∞a v√†o ng·ªØ c·∫£nh -> Ti·∫øt ki·ªám Token ƒë·∫ßu v√†o
    search_context = ""
    if "search_results" in st.session_state and st.session_state.search_results:
        top_results = st.session_state.search_results[:15] 
        search_context = "\n\n[D·ªÆ LI·ªÜU T√åM KI·∫æM T·ª™ B·∫¢N ƒê·ªí]:\n"
        for i, r in enumerate(top_results):
            # L√†m tr√≤n kho·∫£ng c√°ch
            dist = int(r.get('distance_sort', 0))
            # Format ng·∫Øn g·ªçn: T√™n | Gi√° | Lo·∫°i | C√°ch xa
            search_context += f"{i+1}. {r['name']} | Gi√°: {r['price']} | Lo·∫°i: {r['cuisine']} | C√°ch: {dist}m\n"
    else:
        search_context = "\n(Ng∆∞·ªùi d√πng ch∆∞a t√¨m ki·∫øm qu√°n n√†o tr√™n b·∫£n ƒë·ªì)."

    # 4. X√¢y d·ª±ng System Prompt (H∆∞·ªõng d·∫´n h√†nh vi)
    # L∆ØU √ù: Gemma-3 kh√¥ng h·ªó tr·ª£ system_instruction trong config, n√™n ta ph·∫£i g·ªôp v√†o text n√†y.
    system_prompt_text = f"""
    ROLE: B·∫°n l√† "Foodie Guide" - Tr·ª£ l√Ω ·∫©m th·ª±c ƒë·ªãa ph∆∞∆°ng am hi·ªÉu Vi·ªát Nam.
    CONTEXT: {search_context}
    INSTRUCTION:
    - Tr·∫£ l·ªùi ng·∫Øn g·ªçn, th√¢n thi·ªán, d√πng emoji üçú.
    - N·∫øu c√≥ d·ªØ li·ªáu t√¨m ki·∫øm, h√£y ∆∞u ti√™n t∆∞ v·∫•n t·ª´ danh s√°ch ƒë√≥.
    - N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu, h√£y t∆∞ v·∫•n d·ª±a tr√™n ki·∫øn th·ª©c chung v·ªÅ ·∫©m th·ª±c.
    - ƒê·ªãnh d·∫°ng c√¢u tr·∫£ l·ªùi b·∫±ng Markdown d·ªÖ ƒë·ªçc.
    """

    # 5. Giao di·ªán Suggestion Chips (G·ª£i √Ω c√¢u h·ªèi)
    # Ch·ªâ hi·ªán khi ch∆∞a c√≥ l·ªãch s·ª≠ chat
    prompt = None
    if "gemini_messages" not in st.session_state:
        st.session_state["gemini_messages"] = []

    if not st.session_state.gemini_messages:
        # Suggestion header v·ªõi thi·∫øt k·∫ø m·ªõi
        st.markdown(f"""
        <div style="
            background: linear-gradient(135deg, #f0f9ff 0%, #e0f2fe 100%);
            border: 1px solid #bae6fd;
            border-radius: 12px;
            padding: 1rem 1.25rem;
            margin-bottom: 1rem;
        ">
            <div style="
                font-weight: 600;
                color: #0369a1;
                margin-bottom: 0.5rem;
                display: flex;
                align-items: center;
                gap: 0.5rem;
            ">
                <span>üí°</span> {get_text("suggestion_header", lang)}
            </div>
            <div style="color: #64748b; font-size: 0.85rem;">
                Click v√†o g·ª£i √Ω b√™n d∆∞·ªõi ho·∫∑c nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n
            </div>
        </div>
        """, unsafe_allow_html=True)

        # Chips v·ªõi thi·∫øt k·∫ø ƒë·∫πp h∆°n
        cols = st.columns(3)
        if cols[0].button(f"üìä {get_text('chip_analyze', lang)}", use_container_width=True):
            prompt = "D·ª±a tr√™n danh s√°ch c√°c qu√°n t√¨m ƒë∆∞·ª£c, h√£y ph√¢n t√≠ch ∆∞u nh∆∞·ª£c ƒëi·ªÉm c·ªßa ch√∫ng."
        if cols[1].button(f"üç¥ {get_text('chip_side_dish', lang)}", use_container_width=True):
            dish = st.session_state.get('dish_input', 'm√≥n n√†y')
            prompt = f"M√≥n {dish} th∆∞·ªùng ƒÉn k√®m v·ªõi g√¨ cho ƒë√∫ng ƒëi·ªáu?"
        if cols[2].button(f"üí∞ {get_text('chip_cheapest', lang)}", use_container_width=True):
            prompt = "Qu√°n n√†o r·∫ª nh·∫•t v√† g·∫ßn nh·∫•t trong danh s√°ch?"

    # 6. Hi·ªÉn th·ªã L·ªãch s·ª≠ Chat (UI)
    for message in st.session_state.gemini_messages:
        role = "user" if message.role == "user" else "assistant"
        # Tr√≠ch xu·∫•t text an to√†n t·ª´ object Content
        if hasattr(message, 'parts') and len(message.parts) > 0:
            content = message.parts[0].text
        else:
            content = str(message)
            
        with st.chat_message(role):
            st.markdown(content)

    # 7. X·ª≠ l√Ω Input ng∆∞·ªùi d√πng
    user_input = st.chat_input(get_text("chat_placeholder", lang))
    final_prompt = prompt if prompt else user_input

    # 8. Logic G·ª≠i tin nh·∫Øn & G·ªçi API
    if final_prompt:
        if not client:
            st.error("Vui l√≤ng c·∫•u h√¨nh GOOGLE_AI_API_KEY trong .streamlit/secrets.toml")
            return

        # A. Hi·ªÉn th·ªã & L∆∞u tin nh·∫Øn User
        with st.chat_message("user"):
            st.markdown(final_prompt)
        
        user_msg_obj = types.Content(role="user", parts=[types.Part(text=final_prompt)])
        st.session_state.gemini_messages.append(user_msg_obj)

        # B. X·ª≠ l√Ω Chatbot ph·∫£n h·ªìi
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            
            try:
                # --- K·ª∏ THU·∫¨T TI·∫æT KI·ªÜM QUOTA & S·ª¨A L·ªñI 400 ---
                
                # B1. T·∫°o tin nh·∫Øn System gi·∫£ (ƒë√≥ng vai tr√≤ l√† User message ƒë·∫ßu ti√™n)
                # ƒê√¢y l√† c√°ch ƒë·ªÉ Gemma hi·ªÉu ng·ªØ c·∫£nh m√† kh√¥ng c·∫ßn tham s·ªë system_instruction
                sys_msg = types.Content(role="user", parts=[types.Part(text=system_prompt_text)])
                
                # B2. L·∫•y l·ªãch s·ª≠ chat (tr·ª´ tin nh·∫Øn m·ªõi nh·∫•t v·ª´a append ƒë·ªÉ tr√°nh tr√πng l·∫∑p khi gh√©p)
                history = st.session_state.gemini_messages[:-1]
                
                # B3. Sliding Window: Ch·ªâ l·∫•y 6 tin nh·∫Øn g·∫ßn nh·∫•t ƒë·ªÉ g·ª≠i ƒëi
                # Gi√∫p gi·∫£m s·ªë l∆∞·ª£ng token input, ph·∫£n h·ªìi nhanh h∆°n v√† ti·∫øt ki·ªám quota
                if len(history) > 6:
                    history = history[-6:]
                
                # B4. Gh√©p th√†nh danh s√°ch g·ª≠i API: [Lu·∫≠t ch∆°i] + [L·ªãch s·ª≠ ng·∫Øn] + [C√¢u h·ªèi m·ªõi]
                messages_to_send = [sys_msg] + history + [user_msg_obj]

                # B5. G·ªçi API Streaming
                # Model gemma-3-27b-it c√≥ Quota r·∫•t cao (14.4k req/ng√†y)
                response = client.models.generate_content_stream(
                    model="gemma-3-27b-it", 
                    contents=messages_to_send,
                    config=types.GenerateContentConfig(
                        temperature=0.7,
                        max_output_tokens=1500, # Gi·ªõi h·∫°n ƒë·ªô d√†i c√¢u tr·∫£ l·ªùi
                        # QUAN TR·ªåNG: Kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ system_instruction ·ªü ƒë√¢y!
                    )
                )
                    
                for chunk in response:
                    if chunk.text:
                        full_response += chunk.text
                        message_placeholder.markdown(full_response + "‚ñå")
                    
                message_placeholder.markdown(full_response)
                
                # C. L∆∞u c√¢u tr·∫£ l·ªùi c·ªßa Bot v√†o l·ªãch s·ª≠
                assistant_msg_obj = types.Content(role="model", parts=[types.Part(text=full_response)])
                st.session_state.gemini_messages.append(assistant_msg_obj)

            except Exception as e:
                # X·ª≠ l√Ω l·ªói hi·ªÉn th·ªã th√¢n thi·ªán
                error_msg = str(e)
                if "429" in error_msg:
                    st.error("H·ªá th·ªëng ƒëang b·∫≠n (Qu√° t·∫£i Quota). Vui l√≤ng ƒë·ª£i 1 ph√∫t.")
                elif "404" in error_msg:
                    st.error(f"L·ªói Model: Kh√¥ng t√¨m th·∫•y model gemma-3-27b-it. H√£y ki·ªÉm tra l·∫°i t√™n model.")
                else:
                    st.error(f"ƒê√£ x·∫£y ra l·ªói: {e}")